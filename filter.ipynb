{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Parse the profiler output to compare input/output rows and op times for GPU filter operations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38b70794d584d102"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "profile_dirs = [\"/opt/data/profiles/dataproc-gpu-sf100\",\n",
    "                \"/opt/data/profiles/dataproc-gpu-sf3k\",\n",
    "                \"/opt/data/profiles/dataproc-gpu-sf5k\",\n",
    "                \"/opt/data/profiles/dataproc-gpu-sf10k\"]\n",
    "event_dirs = [\"/opt/data/events/dataproc-gpu-sf100\",\n",
    "              \"/opt/data/events/dataproc-gpu-sf3k\",\n",
    "              \"/opt/data/events/dataproc-gpu-sf5k\",\n",
    "              \"/opt/data/events/dataproc-gpu-sf10k\"]\n",
    "scale_factors = [\"100\", \"3K\", \"5K\", \"10K\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bfab33d0f98d9ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Filter:\n",
    "    def __init__(self, group):\n",
    "        self.input_rows = 0\n",
    "        self.output_rows = 0\n",
    "        self.op_time = 0\n",
    "        self.accumulator_id = 0\n",
    "        for row in group.itertuples():\n",
    "            if row.name == \"output rows\":\n",
    "                self.output_rows = row.total\n",
    "            elif row.name == \"op time\":\n",
    "                self.op_time = row.total / 1000000.\n",
    "                self.accumulator_id = row.accumulatorId\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.input_rows}, {self.output_rows}, {self.op_time}, {self.accumulator_id}\"\n",
    "\n",
    "\n",
    "def collect_output_rows(profile_dir):\n",
    "    sql_info = pd.read_csv(f\"{profile_dir}/sql_plan_metrics_for_application.csv\")\n",
    "    condition = sql_info[\"name\"] == \"output rows\"\n",
    "    filtered = sql_info[condition].set_index(\"accumulatorId\")\n",
    "    return filtered[\"total\"].to_dict()\n",
    "\n",
    "\n",
    "def collect_filters(profile_dir):\n",
    "    sql_info = pd.read_csv(f\"{profile_dir}/sql_plan_metrics_for_application.csv\")\n",
    "    condition = sql_info['nodeName'] == 'GpuFilter'\n",
    "    filtered = sql_info[condition]\n",
    "    grouped = filtered.groupby(\"nodeID\")\n",
    "    filter_map = {}\n",
    "    for _, group in grouped:\n",
    "        f = Filter(group)\n",
    "        filter_map[f.accumulator_id] = f\n",
    "    return filter_map\n",
    "\n",
    "\n",
    "def collect(node, filter_list, output_rows_map, filter_map):\n",
    "    if node['nodeName'] == 'GpuFilter':\n",
    "        f = None\n",
    "        for m in node['metrics']:\n",
    "            if m['name'] == 'op time':\n",
    "                accumulator_id = m['accumulatorId']\n",
    "                if accumulator_id in filter_map:\n",
    "                    f = filter_map[accumulator_id]\n",
    "                else:\n",
    "                    print(f\"Accumulator ID {accumulator_id} not found in filter map\")\n",
    "        if f is not None:\n",
    "            child = node['children'][0]\n",
    "            for m in child['metrics']:\n",
    "                if m['name'] == 'output rows':\n",
    "                    accumulator_id = m['accumulatorId']\n",
    "                    if accumulator_id in output_rows_map:\n",
    "                        f.input_rows = output_rows_map[accumulator_id]\n",
    "                    else:\n",
    "                        print(f\"Accumulator ID {f.accumulator_id} not found in output rows map\")\n",
    "                    filter_list.append(f)\n",
    "    for child in node['children']:\n",
    "        collect(child, filter_list, output_rows_map, filter_map)\n",
    "\n",
    "\n",
    "def collect_input_rows(event_file, output_rows_map, filter_map):\n",
    "    last_event = None\n",
    "    with open(event_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            event = json.loads(line)\n",
    "            if \"sparkPlanInfo\" in event:\n",
    "                last_event = event\n",
    "    assert last_event is not None\n",
    "    fs = []\n",
    "    collect(last_event[\"sparkPlanInfo\"], fs, output_rows_map, filter_map)\n",
    "    return fs\n",
    "\n",
    "\n",
    "num_scale_factors = len(scale_factors)\n",
    "input_rows = [[] for _ in range(num_scale_factors)]\n",
    "output_rows = [[] for _ in range(num_scale_factors)]\n",
    "op_times = [[] for _ in range(num_scale_factors)]\n",
    "for i in range(num_scale_factors):\n",
    "    profile_apps = os.listdir(profile_dirs[i])\n",
    "    profile_apps.sort()\n",
    "    event_apps = os.listdir(event_dirs[i])\n",
    "    event_apps.sort()\n",
    "    assert len(profile_apps) == len(event_apps)\n",
    "    for j in range(len(profile_apps)):\n",
    "        print(f\"Processing profile {profile_apps[j]}, event {event_apps[j]}, scale factor {scale_factors[i]}...\")\n",
    "        output_rows_dict = collect_output_rows(f\"{profile_dirs[i]}/{profile_apps[j]}\")\n",
    "        filter_dict = collect_filters(f\"{profile_dirs[i]}/{profile_apps[j]}\")\n",
    "        filters = collect_input_rows(f\"{event_dirs[i]}/{event_apps[j]}\", output_rows_dict, filter_dict)\n",
    "        for fltr in filters:\n",
    "            if fltr.input_rows == 0 or fltr.output_rows == 0 or fltr.op_time == 0:\n",
    "                print(f\"Invalid filter: {fltr}\")\n",
    "            else:\n",
    "                input_rows[i].append(fltr.input_rows)\n",
    "                output_rows[i].append(fltr.output_rows)\n",
    "                op_times[i].append(fltr.op_time)\n",
    "\n",
    "print(f\"{[len(input_rows[i]) for i in range(num_scale_factors)]}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75275cfe783dd3d0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(num_scale_factors):\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(np.log(input_rows[i]), np.log(output_rows[i]), np.log(op_times[i]))\n",
    "    ax.set_xlabel(\"Input Rows (log)\")\n",
    "    ax.set_ylabel(\"Output Rows (log)\")\n",
    "    ax.set_zlabel(\"Op Time (ms) (log)\")\n",
    "    ax.set_title(f\"GPU Filter Op Times - SF{scale_factors[i]}\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bc8bca893683344",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "for i in range(num_scale_factors):\n",
    "    ax.scatter(np.log(input_rows[i]), np.log(output_rows[i]), np.log(op_times[i]), label=f\"SF{scale_factors[i]}\")\n",
    "ax.set_xlabel(\"Input Rows (log)\")\n",
    "ax.set_ylabel(\"Output Rows (log)\")\n",
    "ax.set_zlabel(\"Op Time (ms) (log)\")\n",
    "ax.set_title(f\"GPU Filter Op Times - Combined\")\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8da83b2d22d37261",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
